{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Nu049LV8NlXw",
        "0mucMKobXcbm"
      ],
      "mount_file_id": "1Z0SgygDt534ZCa06mufcy96cssfBim0M",
      "authorship_tag": "ABX9TyNf3iOvEvRRSh89Bzi1DyHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hl5chen/ECE1512-2022F-ProjectRepo_YiyangShi_HsuanlingChen/blob/main/Project_B_Supp/ECE1512_ProjectB_Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Dataset Distillation with Gradient Matching\n",
        "\n",
        "In this task, the dataset distillation will be used with gradient matching to learn a synthetically small dataset for the MNIST and CIFAR10 dataset, train networks from scratch on the condensed images, and then evaluate them on the real testing data. \n",
        "\n"
      ],
      "metadata": {
        "id": "S4xePafyM6LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dependencies and Setup "
      ],
      "metadata": {
        "id": "Nu049LV8NlXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install import-ipynb\n",
        "import import_ipynb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "JG_ZlrrdSH_g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khq5FvqrXKEg",
        "outputId": "59151739-c361-481b-8a7b-9c053eba8da6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNZc_3nyXN6P",
        "outputId": "4561e205-cbaa-4052-f941-4ab98539cad9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECE1512_ProjectB_Task1.ipynb  networks.ipynb  utils.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Colab Notebooks/ECE1512_F22/ProjectB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZsN0HoxVQvR",
        "outputId": "16bf68e7-4cbf-48c5-bf78-9b60a20d17c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks/ECE1512_F22/ProjectB'\n",
            "/content/drive/MyDrive/Colab Notebooks/ECE1512_F22/ProjectB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define HyperParameters and Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "0mucMKobXcbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import sys\n",
        "#sys.path.append('/drive/MyDrive/Colab Notebooks/ECE1512_F22/ProjectB')\n",
        "\n",
        "import networks\n",
        "import utils\n",
        "import numpy as np\n",
        "\n",
        "BATCH_SIZE = 256 # size of input data took for one iteration\n",
        "NUM_EPOCHS = 50 # number of times which the entire dataset is passed throughout the model\n",
        "NUM_CLASSES = 10  # 10 total classes.\n",
        "LEARNING_RATE = 1e-3 # size of step "
      ],
      "metadata": {
        "id": "BejJlAMaS5MT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load MNIST Data"
      ],
      "metadata": {
        "id": "kefCr6ZkNq7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test splits.\n",
        "mnist_train = dsets.MNIST(root = './data', train = True, transform = transforms.ToTensor(), download = True)\n",
        "mnist_test = dsets.MNIST(root = './data', train = False, transform = transforms.ToTensor())\n",
        "\n",
        "mnist_train_gen = torch.utils.data.DataLoader(dataset = mnist_train, batch_size = BATCH_SIZE, shuffle = True)\n",
        "mnist_test_gen = torch.utils.data.DataLoader(dataset = mnist_test, batch_size = BATCH_SIZE, shuffle = False)"
      ],
      "metadata": {
        "id": "AY8CtncqXoZW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Creation "
      ],
      "metadata": {
        "id": "MWSUm5R5Op1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from networks import MLP\n",
        "\n",
        "#self, channel, num_classes\n",
        "MLP = MLP(self, channel, NUM_CLASSES)\n",
        "if torch.cuda.is_available():\n",
        "  MLP.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "DA53-92BO1dT",
        "outputId": "a7855efc-c355-4629-8acf-43e5e576a9f6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-8b9abe5183ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#self, channel, num_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mMLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mMLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "Xz-0JI84aDHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD( MLP.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "r1EFgP_zaC11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model "
      ],
      "metadata": {
        "id": "avhv9Tn8gq31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inner Loop"
      ],
      "metadata": {
        "id": "0TEc1EPZjro2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0s2hiKsjsnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model"
      ],
      "metadata": {
        "id": "AadEkxWogx-k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5ybsdC9g2w9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}