{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WYMfvCNPwpm"
      },
      "source": [
        "# Project A: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA8ppgB2P0aJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from typing import Union\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "tf.keras.utils.set_random_seed(1)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "builder = tfds.builder('mnist')\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 12 # 12\n",
        "NUM_CLASSES = 10  # 10 total classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2EFLQROP2R7"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynByMG_UP4A4"
      },
      "outputs": [],
      "source": [
        "# Load train and test splits.\n",
        "def preprocess(x):\n",
        "    image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
        "    subclass_labels = tf.one_hot(x['label'], builder.info.features['label'].num_classes)\n",
        "    return image, subclass_labels\n",
        "\n",
        "\n",
        "mnist_train = tfds.load('mnist', split='train', shuffle_files=False).cache()\n",
        "mnist_train = mnist_train.map(preprocess)\n",
        "mnist_train = mnist_train.shuffle(builder.info.splits['train'].num_examples)\n",
        "mnist_train = mnist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "mnist_test = tfds.load('mnist', split='test').cache()\n",
        "mnist_test = mnist_test.map(preprocess).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAZwfvW5P63q"
      },
      "source": [
        "# Model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zINgDkA7P7BP"
      },
      "outputs": [],
      "source": [
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "# Build CNN teacher.\n",
        "import tensorflow.compat.v2.keras.layers as layers\n",
        "\n",
        "\n",
        "def initialize_cnn():\n",
        "  cnn_model = tf.keras.Sequential()\n",
        "\n",
        "  # your code start from here for stpe 2\n",
        "  cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "  cnn_model.add(layers.MaxPooling2D((2, 2), strides=(1, 1)))\n",
        "  cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  cnn_model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "  cnn_model.add(layers.Flatten())\n",
        "  cnn_model.add(layers.Dropout(0.5))\n",
        "  cnn_model.add(layers.Dense(128, activation='relu'))\n",
        "  cnn_model.add(layers.Dropout(0.5))\n",
        "  cnn_model.add(layers.Dense(10))\n",
        "  return cnn_model\n",
        "\n",
        "\n",
        "\n",
        "def initialize_fc():\n",
        "  # Build fully connected student.\n",
        "  fc_model = tf.keras.Sequential()\n",
        "  fc_model.add(layers.Flatten())\n",
        "  fc_model.add(layers.Dense(784, activation='relu'))\n",
        "  fc_model.add(layers.Dense(784, activation='relu'))\n",
        "  fc_model.add(layers.Dense(10))\n",
        "  return fc_model\n",
        "\n",
        "cnn_model = initialize_cnn()\n",
        "fc_model = initialize_fc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JWGucyrQGav"
      },
      "source": [
        "# Teacher loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhzBP6ZLQJ57"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def compute_teacher_loss(images, labels):\n",
        "    \"\"\"Compute subclass knowledge distillation teacher loss for given images\n",
        "     and labels.\n",
        "\n",
        "    Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "    Returns:\n",
        "    Scalar loss Tensor.\n",
        "    \"\"\"\n",
        "    subclass_logits = cnn_model(images, training=True)\n",
        "\n",
        "    # your code start from here for step 3\n",
        "\n",
        "    cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=subclass_logits)\n",
        "\n",
        "    return tf.reduce_mean(cross_entropy_loss_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS8xkuH0QbOS"
      },
      "source": [
        "# Student loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDKia4gPQMIr"
      },
      "outputs": [],
      "source": [
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "# Hyperparameters for distillation (need to be tuned).\n",
        "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
        "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "    \"\"\"Compute distillation loss.\n",
        "\n",
        "    This function computes cross entropy between softened logits and softened\n",
        "    targets. The resulting loss is scaled by the squared temperature so that\n",
        "    the gradient magnitude remains approximately constant as the temperature is\n",
        "    changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
        "    a neural network.\"\n",
        "\n",
        "    Args:\n",
        "    teacher_logits: A Tensor of logits provided by the teacher.\n",
        "    student_logits: A Tensor of logits provided by the student, of the same\n",
        "      shape as `teacher_logits`.\n",
        "    temperature: Temperature to use for distillation.\n",
        "\n",
        "    Returns:\n",
        "    A scalar Tensor containing the distillation loss.\n",
        "    \"\"\"\n",
        "    # your code start from here for step 3\n",
        "    soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
        "    '''\n",
        "    kl = tf.keras.losses.KLDivergence()\n",
        "\n",
        "\n",
        "    return tf.reduce_mean(kl(soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "    '''\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "def compute_student_loss(images, labels):\n",
        "    \"\"\"Compute subclass knowledge distillation student loss for given images\n",
        "     and labels.\n",
        "\n",
        "    Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "    Returns:\n",
        "    Scalar loss Tensor.\n",
        "    \"\"\"\n",
        "    student_subclass_logits = fc_model(images, training=True)\n",
        "\n",
        "    # Compute subclass distillation loss between student subclass logits and\n",
        "    # softened teacher subclass targets probabilities.\n",
        "\n",
        "    # your code start from here for step 3\n",
        "\n",
        "    teacher_subclass_logits = cnn_model(images, training=False)\n",
        "\n",
        "    student_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=student_subclass_logits)\n",
        "    student_loss_value = tf.reduce_mean(student_loss_value)\n",
        "\n",
        "    distillation_loss_value = distillation_loss(teacher_subclass_logits, student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
        "    # Compute cross-entropy loss with hard targets.\n",
        "\n",
        "    # your code start from here for step 3\n",
        "    student_loss = distillation_loss_value * ALPHA + student_loss_value * (1 - ALPHA)\n",
        "    #print(distillation_loss_value.numpy())\n",
        "\n",
        "    return student_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ1uyvurQ3w4"
      },
      "source": [
        "# Train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtoLbp8uQ4Vl"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def compute_num_correct(model, images, labels):\n",
        "    \"\"\"Compute number of correctly classified images in a batch.\n",
        "\n",
        "    Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "    Returns:\n",
        "    Number of correctly classified images.\n",
        "    \"\"\"\n",
        "    class_logits = model(images, training=False)\n",
        "    return tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32))\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn, compute_num_correct):\n",
        "    \"\"\"Perform training and evaluation for a given model.\n",
        "\n",
        "    Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    compute_loss_fn: A function that computes the training loss given the\n",
        "      images, and labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # your code start from here for step 4\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        # Run training.\n",
        "        print('Epoch {}: '.format(epoch), end='')\n",
        "        for images, labels in mnist_train:\n",
        "            with tf.GradientTape() as tape:\n",
        "                # your code start from here for step 4\n",
        "                loss_value = compute_loss_fn(images, labels)\n",
        "                #print(\"loss value \" + str(loss_value.numpy()))\n",
        "\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Run evaluation.\n",
        "        num_correct = 0\n",
        "        num_total = builder.info.splits['test'].num_examples\n",
        "    \n",
        "        for images, labels in mnist_test:\n",
        "            # your code start from here for step 4\n",
        "            num_correct += compute_num_correct(model, images, labels)\n",
        "\n",
        "        print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "            num_correct / num_total * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQL1lJdaRPT1"
      },
      "source": [
        "# Training models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "-AGHbyABRPz3",
        "outputId": "7be10740-b66c-4258-aee0-e4807f597277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------Teacher-------\n",
            "Epoch 1: Class_accuracy: 99.26%\n",
            "Epoch 2: Class_accuracy: 99.21%\n",
            "Epoch 3: Class_accuracy: 99.35%\n",
            "Epoch 4: Class_accuracy: 99.25%\n",
            "Epoch 5: Class_accuracy: 99.31%\n",
            "Epoch 6: Class_accuracy: 99.29%\n",
            "Epoch 7: Class_accuracy: 99.35%\n",
            "Epoch 8: Class_accuracy: 99.32%\n",
            "Epoch 9: Class_accuracy: 99.33%\n",
            "Epoch 10: Class_accuracy: 99.27%\n",
            "Epoch 11: Class_accuracy: 99.39%\n",
            "Epoch 12: Class_accuracy: 99.38%\n",
            "------Student-------\n",
            "Epoch 1: "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-806997d9915b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_teacher_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_num_correct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------Student-------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_student_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_num_correct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-0dfa8e6fc0fa>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, compute_loss_fn, compute_num_correct)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Run training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {}: '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmnist_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# your code start from here for step 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3012\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3013\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   3015\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# your code start from here for step 5 \n",
        "print(\"------Teacher-------\")\n",
        "train_and_evaluate(cnn_model, compute_teacher_loss, compute_num_correct)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"------Student-------\")\n",
        "train_and_evaluate(fc_model, compute_student_loss, compute_num_correct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC9D2wQbktAx",
        "outputId": "6b139add-fc57-433b-ff62-f419400906d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------Student-------\n",
            "Epoch 1: Class_accuracy: 97.42%\n",
            "Epoch 2: Class_accuracy: 98.18%\n",
            "Epoch 3: Class_accuracy: 98.26%\n",
            "Epoch 4: Class_accuracy: 98.54%\n",
            "Epoch 5: Class_accuracy: 98.66%\n",
            "Epoch 6: Class_accuracy: 98.62%\n",
            "Epoch 7: Class_accuracy: 98.71%\n",
            "Epoch 8: Class_accuracy: 98.75%\n",
            "Epoch 9: Class_accuracy: 98.86%\n",
            "Epoch 10: Class_accuracy: 98.80%\n",
            "Epoch 11: Class_accuracy: 98.85%\n",
            "Epoch 12: Class_accuracy: 98.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj1N38fnRTNB"
      },
      "source": [
        "# Test accuracy vs. tempreture curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX4dbazrRWIz"
      },
      "outputs": [],
      "source": [
        "# your code start from here for step 6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNrH_1emRbGA"
      },
      "source": [
        "# Train student from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjospsxIRbQ6"
      },
      "outputs": [],
      "source": [
        "# Build fully connected student.\n",
        "fc_model_no_distillation = tf.keras.Sequential()\n",
        "\n",
        "# your code start from here for step 7\n",
        "\n",
        "\n",
        "\n",
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "def compute_plain_cross_entropy_loss(images, labels):\n",
        "  \"\"\"Compute plain loss for given images and labels.\n",
        "\n",
        "  For fair comparison and convenience, this function also performs a\n",
        "  LogSumExp over subclasses, but does not perform subclass distillation.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  # your code start from here for step 7\n",
        "\n",
        "  student_subclass_logits = fc_model_no_distillation(images, training=True)\n",
        "  cross_entropy_loss = \n",
        "  \n",
        "  return cross_entropy_loss\n",
        "\n",
        "\n",
        "train_and_evaluate(fc_model_no_distillation, compute_plain_cross_entropy_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq3JTpQ4RuhR"
      },
      "source": [
        "# Comparing the teacher and student model (number of of parameters and FLOPs) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V8GB2yRRuxF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "3e848c45-7350-4df8-e883-177100203dfc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f90c8e73b4ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# your code start from here for step 8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_flops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_flops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# use get_flops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_flops'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# your code start from here for step 8\n",
        "from keras_flops import get_flops\n",
        "# use get_flops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjwJ5oziRvRn"
      },
      "source": [
        "# Implementing the state-of-the-art KD algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q10lybAFRvZt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# your code start from here for step 12\n",
        "class CNN_SubClass:\n",
        "  def __init__(self, num_of_subclass, temperature, beta):\n",
        "    self.num_of_subclass = num_of_subclass\n",
        "    self.temperature = temperature\n",
        "    self.beta = beta\n",
        "    self.model = tf.keras.Sequential()\n",
        "\n",
        "    # your code start from here for stpe 2\n",
        "    self.model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    self.model.add(layers.MaxPooling2D((2, 2), strides=(1, 1)))\n",
        "    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    self.model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    self.model.add(layers.Flatten())\n",
        "    self.model.add(layers.Dropout(0.5))\n",
        "    self.model.add(layers.Dense(128, activation='relu'))\n",
        "    self.model.add(layers.Dropout(0.5))\n",
        "    self.model.add(layers.Dense(10*num_of_subclass))\n",
        "  \n",
        "  def loss_function(self, images, labels):\n",
        "\n",
        "    subclass_logits = self.model(images, training=True)\n",
        "    # tf.shape(subclass_logits)[0] is batch size \n",
        "    # sum all subclass of a class, get back size 256 x 10 logits \n",
        "    class_logits = tf.reduce_sum(tf.reshape(subclass_logits, (tf.shape(subclass_logits)[0], 10, self.num_of_subclass)),axis=2)\n",
        "    cross_entropy_loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=class_logits))\n",
        "\n",
        "\n",
        "    mean = tf.math.reduce_mean(subclass_logits, axis=1)\n",
        "    var = tf.math.reduce_variance(subclass_logits, axis=1)\n",
        "\n",
        "\n",
        "    \n",
        "    normalize = tf.keras.layers.Normalization(axis=0, mean=mean, variance=var)\n",
        "    normalized_logits = normalize(subclass_logits)\n",
        "  \n",
        "    # dot product to get to 256 by 256 size \n",
        "    # input 256 * 20, so M dot M.T\n",
        "    \n",
        "    # summing all rows to get exponential sum \n",
        "    exp_sum = tf.reduce_sum(\n",
        "        #dot product of normalized logit with it self\n",
        "        tf.math.exp(tf.matmul(\n",
        "            normalized_logits, \n",
        "            tf.transpose(normalized_logits)) / self.temperature),\n",
        "            axis=1)\n",
        "  \n",
        "    \n",
        "    log_avg = tf.reduce_mean(tf.math.log(exp_sum))\n",
        "    # n = batch size \n",
        "    aux_loss = log_avg - 1 / self.temperature - np.log(tf.shape(subclass_logits)[0])\n",
        "    #print(aux_loss)\n",
        "\n",
        "    total_loss = cross_entropy_loss_value  + self.beta * aux_loss\n",
        "\n",
        "    #print(total_loss)\n",
        "\n",
        "    return total_loss \n",
        "  \n",
        "  def compute_num_correct(self, model, images, labels):\n",
        "    subclass_logits = model(images, training=False)\n",
        "    # tf.shape(subclass_logits)[0] is batch size \n",
        "    class_logits = tf.reduce_sum(tf.reshape(subclass_logits, (tf.shape(subclass_logits)[0], 10, self.num_of_subclass)),axis=2)\n",
        "    \n",
        "    return tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32))\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FC_SubClass:\n",
        "  def __init__(self, num_of_subclass, temperature, alpha):\n",
        "    self.num_of_subclass = num_of_subclass\n",
        "    self.temperature = temperature\n",
        "    self.alpha = alpha\n",
        "    self.model = tf.keras.Sequential()\n",
        "\n",
        "    # your code start from here for stpe 2\n",
        "    self.model = tf.keras.Sequential()\n",
        "    self.model.add(layers.Flatten())\n",
        "    self.model.add(layers.Dense(784, activation='relu'))\n",
        "    self.model.add(layers.Dense(784, activation='relu'))\n",
        "    self.model.add(layers.Dense(10*num_of_subclass))\n",
        "\n",
        "  def compute_num_correct(self, model, images, labels):\n",
        "    subclass_logits = model(images, training=False)\n",
        "    # tf.shape(subclass_logits)[0] is batch size \n",
        "    class_logits = tf.reduce_sum(tf.reshape(subclass_logits, (tf.shape(subclass_logits)[0], 10, self.num_of_subclass)),axis=2)\n",
        "    \n",
        "    return tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32))\n",
        "    \n",
        "\n",
        "  def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "    \n",
        "    soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "    \n",
        "  def loss_function(self, images, labels, teacher_model):\n",
        "\n",
        "    student_subclass_logits = self.model(images, training=True)\n",
        "\n",
        "    class_logits = tf.reduce_sum(tf.reshape(student_subclass_logits, (tf.shape(student_subclass_logits)[0], 10, self.num_of_subclass)),axis=2)\n",
        "\n",
        "    # Compute subclass distillation loss between student subclass logits and\n",
        "    # softened teacher subclass targets probabilities.\n",
        "\n",
        "    # your code start from here for step 3\n",
        "\n",
        "    teacher_subclass_logits = teacher_model(images, training=False)\n",
        "\n",
        "    student_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=class_logits)\n",
        "    student_loss_value = tf.reduce_mean(student_loss_value)\n",
        "\n",
        "    distillation_loss_value = distillation_loss(teacher_subclass_logits, student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
        "    # Compute cross-entropy loss with hard targets.\n",
        "\n",
        "    # your code start from here for step 3\n",
        "    student_loss = distillation_loss_value * ALPHA + student_loss_value * (1 - ALPHA)\n",
        "\n",
        "    return student_loss \n",
        "  \n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "i5bFRwPEYWet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_student(model, teacher_model, compute_loss_fn, compute_num_correct):\n",
        "    \"\"\"Perform training and evaluation for a given model.\n",
        "\n",
        "    Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    compute_loss_fn: A function that computes the training loss given the\n",
        "      images, and labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # your code start from here for step 4\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        # Run training.\n",
        "        print('Epoch {}: '.format(epoch), end='')\n",
        "        for images, labels in mnist_train:\n",
        "            with tf.GradientTape() as tape:\n",
        "                # your code start from here for step 4\n",
        "                loss_value = compute_loss_fn(images, labels, teacher_model)\n",
        "                #print(\"loss value \" + str(loss_value.numpy()))\n",
        "\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Run evaluation.\n",
        "        num_correct = 0\n",
        "        num_total = builder.info.splits['test'].num_examples\n",
        "    \n",
        "        for images, labels in mnist_test:\n",
        "            # your code start from here for step 4\n",
        "            num_correct += compute_num_correct(model, images, labels)\n",
        "\n",
        "        print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "            num_correct / num_total * 100))"
      ],
      "metadata": {
        "id": "cvHlXVIJa7-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_cnn = CNN_SubClass(2, 20, 0.3)\n",
        "train_and_evaluate(updated_cnn.model, updated_cnn.loss_function, updated_cnn.compute_num_correct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSWqaTp6YUDu",
        "outputId": "f2328d1c-fa1a-4a5b-c228-9a809d6f4be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Class_accuracy: 98.19%\n",
            "Epoch 2: Class_accuracy: 98.61%\n",
            "Epoch 3: Class_accuracy: 98.84%\n",
            "Epoch 4: Class_accuracy: 99.03%\n",
            "Epoch 5: Class_accuracy: 99.11%\n",
            "Epoch 6: Class_accuracy: 99.14%\n",
            "Epoch 7: Class_accuracy: 99.17%\n",
            "Epoch 8: Class_accuracy: 99.19%\n",
            "Epoch 9: Class_accuracy: 99.26%\n",
            "Epoch 10: Class_accuracy: 99.16%\n",
            "Epoch 11: Class_accuracy: 99.27%\n",
            "Epoch 12: Class_accuracy: 99.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_fc = FC_SubClass(2, 4, 0.5)\n",
        "train_and_evaluate_student(updated_fc.model, updated_cnn.model, updated_fc.loss_function, updated_fc.compute_num_correct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affAeAxRbhrx",
        "outputId": "107139c2-5ed8-48b5-a7dd-c0abd562906c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Class_accuracy: 97.41%\n",
            "Epoch 2: Class_accuracy: 98.31%\n",
            "Epoch 3: Class_accuracy: 98.44%\n",
            "Epoch 4: Class_accuracy: 98.68%\n",
            "Epoch 5: Class_accuracy: 98.62%\n",
            "Epoch 6: Class_accuracy: 98.78%\n",
            "Epoch 7: Class_accuracy: 98.72%\n",
            "Epoch 8: Class_accuracy: 98.72%\n",
            "Epoch 9: Class_accuracy: 98.83%\n",
            "Epoch 10: Class_accuracy: 98.85%\n",
            "Epoch 11: Class_accuracy: 98.87%\n",
            "Epoch 12: Class_accuracy: 98.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dsOmtqdieIC"
      },
      "source": [
        "# (Optional) XAI method to explain models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0IMIFW8ilPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026a6a87-f007-416b-8242-e3b2a027194a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.18755388, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# your code start from here for step 13\n",
        "import numpy as np\n",
        "#Test loss function \n",
        "\n",
        "x = tf.random.uniform(shape=[500, 80], dtype=tf.float32)\n",
        "y = tf.reduce_sum(\n",
        "    tf.reshape(x, (500, 40, 2)),\n",
        "    axis=2\n",
        ")\n",
        "#print(y)\n",
        "mean = tf.math.reduce_mean(y, axis=1)\n",
        "#print(mean)\n",
        "var = tf.math.reduce_variance(y, axis=1)\n",
        "#print(var)\n",
        "layer = tf.keras.layers.Normalization(axis= 0, mean=mean, variance=var)\n",
        "z = layer(y)\n",
        "\n",
        "T = 10\n",
        "\n",
        "exp_sum = tf.reduce_sum(\n",
        "    tf.math.exp(tf.tensordot(z, tf.transpose(z), 1) / T),\n",
        "    axis=1)\n",
        "\n",
        "log_avg = tf.reduce_mean(tf.math.log(exp_sum))\n",
        "# n = batch size \n",
        "aux_loss = log_avg - 1/T - np.log(500)\n",
        "print(aux_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = [[  0.21940991, 1.821567], [-0.03301997,  1.5130547]]\n",
        "\n",
        "b = [[  0.21940991, 1.821567], [-0.03301997,  1.5130547]]\n",
        "\n",
        "\n",
        "\n",
        "c = np.dot(a,np.transpose(b))\n",
        "\n",
        "a_t = tf.constant(a)\n",
        "b_t = tf.constant(b)\n",
        "\n",
        "print(c)\n",
        "print(tf.tensordot(a_t, tf.transpose(b_t),1))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlLO_VoywdQg",
        "outputId": "9fc0ce4f-fd06-4505-8512-befb479be2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.36624704 2.7488856 ]\n",
            " [2.7488856  2.29042484]]\n",
            "tf.Tensor(\n",
            "[[3.3662474 2.7488859]\n",
            " [2.7488859 2.2904248]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}